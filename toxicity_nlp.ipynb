{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e9e747",
   "metadata": {},
   "source": [
    "# Toxicity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253ba8d",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Build toxicity classification model to demonstarte transfer learning, custom-built model, improvement strategies and the relevant NLP and deep learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940df7b",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "The tocicity classification is a common NLP problem. A toxicity dataset is available in Tensorflow. It is a problem easy to understand and sufficient to demonstrate certain key NLP and Deep Learning techniques, including but not limitted to transfer learning, text embedding, and neural network architectures. We will build transfer learning model and custom-built models. Interesting performances will be shown but our focus will be more on the model architectures. The model architecutres presented here will not only be applicable to this problem, but also have a wide range of applications, which we will discuss in the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07cab03",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Data and problem description.\n",
    "2. Use Tensorflow's pretrained to train a toxicity classification model with the Tensorflow NLP dataset.  \n",
    "3. Build a basic model from scratch and compare the performance.\n",
    "4. Propose a improvement strategy, build the model and compare the performance.\n",
    "5. Discussion - Application & Improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005801d",
   "metadata": {},
   "source": [
    "## Data and problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c0469",
   "metadata": {},
   "source": [
    "The dataset 'wikipedia_toxicity_subtypes' is chosen from Tensorflow NLP dataset. We will use the text and toxicity fields, where toxicity can be either 1 or 0, indicating whether the text is toxic or not. We will build a binary classification out of it for toxicity detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fe2c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Data Processing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2713182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 00:24:57.802094: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 9)\n",
      "b'en'    159571\n",
      "Name: language, dtype: int64\n",
      "0.0    144277\n",
      "1.0     15294\n",
      "Name: toxicity, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"\\nThanks Xeno. -  • Talk • \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009 (UTC)\\nFixed    03:36, 8 June</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question\\nWhat was wrong with the repair I did?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree myself now, actually. (Amazing how the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kisumu \\n\\nI saw that you contributed to Kisum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                      \"\\nThanks Xeno. -  • Talk • \"      0\n",
       "1                 2009 (UTC)\\nFixed    03:36, 8 June      0\n",
       "2    Question\\nWhat was wrong with the repair I did?      0\n",
       "3  I agree myself now, actually. (Amazing how the...      0\n",
       "4  Kisumu \\n\\nI saw that you contributed to Kisum...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "ds = tfds.load('wikipedia_toxicity_subtypes', split='train')\n",
    "ds = tfds.as_dataframe(ds)\n",
    "\n",
    "#Basic data information\n",
    "print(ds.shape)\n",
    "print(ds.language.value_counts())\n",
    "print(ds.toxicity.value_counts())\n",
    "\n",
    "#Encode the Label to convert it into numerical values [Fake = 0; Real = 1]\n",
    "lab_enc = LabelEncoder()\n",
    "\n",
    "#Applying to the dataset\n",
    "ds['label'] = lab_enc.fit_transform(ds['toxicity'])\n",
    "\n",
    "#Decode text\n",
    "ds['text'] = ds['text'].str.decode(\"utf-8\")\n",
    "ds = ds[['text', 'label']]\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b7c6a",
   "metadata": {},
   "source": [
    "### Remark\n",
    "In the above, certain data cleaning technique can be used, e.g. using the text processing package texthero's clean function will easily clean the text. It may not yield better result though. This can be one of future areas to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb96db",
   "metadata": {},
   "source": [
    "## Use Tensorflow's pretrained to train a toxicity classification model with the Tensorflow NLP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6312c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "x_train,x_test,y_train,y_test = train_test_split(ds['text'], ds.label, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0d440d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Trained Text Embedding Model & Layer Definition\n",
    "Embed = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
    "Trainable_Module = True\n",
    "hub_layer = hub.KerasLayer(Embed, input_shape=[], dtype=tf.string, trainable=Trainable_Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c735cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)           #pre-trained text embedding layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1b153ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Model Summary --\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 20)                400020    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                336       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\" -- Model Summary --\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bb9ac",
   "metadata": {},
   "source": [
    "### Description of the model\n",
    "The pre-trained tensorflow model is a small model which has been trained 130GB corpus (20,000 vocabulary) and has 20 dimensions, using Swivel matrix factorization method. On top of the pre-trained model, add a dense layer of 16 dimension, and then the output layer of 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05b23f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compile\n",
    "import tensorflow_addons as tfa\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0fe32e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9718 - val_loss: 0.2132 - val_accuracy: 0.9414\n",
      "Epoch 2/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0670 - accuracy: 0.9728 - val_loss: 0.2173 - val_accuracy: 0.9419\n",
      "Epoch 3/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0647 - accuracy: 0.9742 - val_loss: 0.2238 - val_accuracy: 0.9394\n",
      "Epoch 4/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0627 - accuracy: 0.9748 - val_loss: 0.2320 - val_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0608 - accuracy: 0.9760 - val_loss: 0.2353 - val_accuracy: 0.9406\n",
      "Epoch 6/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0584 - accuracy: 0.9767 - val_loss: 0.2447 - val_accuracy: 0.9414\n",
      "Epoch 7/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0566 - accuracy: 0.9775 - val_loss: 0.2490 - val_accuracy: 0.9397\n",
      "Epoch 8/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0546 - accuracy: 0.9786 - val_loss: 0.2558 - val_accuracy: 0.9409\n",
      "Epoch 9/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0523 - accuracy: 0.9794 - val_loss: 0.2670 - val_accuracy: 0.9404\n",
      "Epoch 10/10\n",
      "505/505 [==============================] - 2s 4ms/step - loss: 0.0506 - accuracy: 0.9806 - val_loss: 0.2693 - val_accuracy: 0.9396\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10           \n",
    "BATCH_SIZE = 256      \n",
    "\n",
    "history = model.fit(x_train,y_train, batch_size = BATCH_SIZE,\n",
    "                    epochs = EPOCHS, validation_split= 0.1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23316c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+\n",
      "|               |  LOSS  | ACCURACY |\n",
      "+---------------+--------+----------+\n",
      "| Model Trained | 30.21% |  93.78%  |\n",
      "+---------------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test,y_test, verbose=0)\n",
    "tab_data = [ [ \"Model Trained\", '{:.2%}'.format(accr[0]), '{:.2%}'.format(accr[1]) ]]\n",
    "print(tabulate(tab_data, headers=['','LOSS','ACCURACY'], tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c61889",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(el[0] > 0.5) for el in model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160ea59",
   "metadata": {},
   "source": [
    "### Performance Metric\n",
    "The above accuracy is not sufficient enough. Need the following precision, recall and f1-score to better understand the preformance. I will choose the two f1-scores (0 and 1) for the comparison between models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4d832fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97     14422\n",
      "           1       0.71      0.60      0.65      1536\n",
      "\n",
      "    accuracy                           0.94     15958\n",
      "   macro avg       0.83      0.79      0.81     15958\n",
      "weighted avg       0.93      0.94      0.94     15958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168d26e",
   "metadata": {},
   "source": [
    "## Build a basic model from scratch and compare the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d7e52",
   "metadata": {},
   "source": [
    "### Model Achitecture\n",
    "The module 'dnn_dense_plus_sparse_module' is a deep learning architure I built using Tensorflow. It can process sparse features, dense features, build neural network layers, and combine the sparse and dense features. In this section, we will focus on using the sparse (TFIDF) features of the text. Inside the module, sparse matrix multiplication is implemented using Tensorflow, so it's fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501b743",
   "metadata": {},
   "source": [
    "The neural network structure is:\n",
    "TFIDF Input -> Hidden Layer (SPARSE_HIDDEN_DIM1=500 specified below) -> Logits (Dimension 2) -> Output (produced by softmax on logits, so it's of dimension 2, and each represents the probability of 0 or 1). \n",
    "\n",
    "Note: This structured is defined in dnn_dense_plus_sparse_module.sparse_graph_sparseOnly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e8e403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dnn_dense_plus_sparse_module import DnnModel\n",
    "from dnn_dense_plus_sparse_module import CreateFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190f966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pre-trained TFIDF Vectorizer\n",
      "Get The TFIDF of Texts\n"
     ]
    }
   ],
   "source": [
    "root_path = '/Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/'\n",
    "\n",
    "# The CreateFeatures class of the custom-built module can create a TFIDF model and generate TFIDF for given text\n",
    "cf = CreateFeatures()\n",
    "cf.tfidf_fit(ds.text, root_path + '/objects/tfidf.pkl')\n",
    "vecs = cf.generate_tfidf_from_text(ds.text, root_path + '/objects/tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59435f",
   "metadata": {},
   "source": [
    "Prepare train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f14d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot target variables\n",
    "targets = (pd.get_dummies(ds.label)).to_numpy()\n",
    "\n",
    "# Split train and test\n",
    "import random\n",
    "random.seed(0)\n",
    "n = len(ds)\n",
    "test_idx = sorted(random.sample(range(n), int(0.2*n)))\n",
    "train_idx = sorted(list(set(range(n)) - set(test_idx)))\n",
    "\n",
    "vecs_train = vecs[train_idx]\n",
    "vecs_test = vecs[test_idx]\n",
    "trainY = targets[train_idx, :]\n",
    "testY = targets[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a73530c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph started running...\n",
      "There will be 3 epochs. Each eopch will have 7979 steps.\n",
      "Average loss at Epoch 0 and Step 499 is: 1.807281\n",
      "Average loss at Epoch 0 and Step 999 is: 1.199665\n",
      "Average loss at Epoch 0 and Step 1499 is: 0.974012\n",
      "Average loss at Epoch 0 and Step 1999 is: 0.859267\n",
      "Average loss at Epoch 0 and Step 2499 is: 0.702072\n",
      "Average loss at Epoch 0 and Step 2999 is: 0.645045\n",
      "Average loss at Epoch 0 and Step 3499 is: 0.620844\n",
      "Average loss at Epoch 0 and Step 3999 is: 0.575954\n",
      "Average loss at Epoch 0 and Step 4499 is: 0.506442\n",
      "Average loss at Epoch 0 and Step 4999 is: 0.518207\n",
      "Average loss at Epoch 0 and Step 5499 is: 0.499323\n",
      "Average loss at Epoch 0 and Step 5999 is: 0.415307\n",
      "Average loss at Epoch 0 and Step 6499 is: 0.409736\n",
      "Average loss at Epoch 0 and Step 6999 is: 0.378404\n",
      "Average loss at Epoch 0 and Step 7499 is: 0.352180\n",
      "Model saved to: /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_sparse221028\n",
      "Average loss at Epoch 1 and Step 20 is: 0.008309\n",
      "Average loss at Epoch 1 and Step 520 is: 0.301501\n",
      "Average loss at Epoch 1 and Step 1020 is: 0.285836\n",
      "Average loss at Epoch 1 and Step 1520 is: 0.264935\n",
      "Average loss at Epoch 1 and Step 2020 is: 0.245360\n",
      "Average loss at Epoch 1 and Step 2520 is: 0.227105\n",
      "Average loss at Epoch 1 and Step 3020 is: 0.222872\n",
      "Average loss at Epoch 1 and Step 3520 is: 0.222011\n",
      "Average loss at Epoch 1 and Step 4020 is: 0.230605\n",
      "Average loss at Epoch 1 and Step 4520 is: 0.220777\n",
      "Average loss at Epoch 1 and Step 5020 is: 0.225986\n",
      "Average loss at Epoch 1 and Step 5520 is: 0.242066\n",
      "Average loss at Epoch 1 and Step 6020 is: 0.231622\n",
      "Average loss at Epoch 1 and Step 6520 is: 0.222464\n",
      "Average loss at Epoch 1 and Step 7020 is: 0.232133\n",
      "Average loss at Epoch 1 and Step 7520 is: 0.242150\n",
      "Model saved to: /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_sparse221028\n",
      "Average loss at Epoch 2 and Step 41 is: 0.018404\n",
      "Average loss at Epoch 2 and Step 541 is: 0.241620\n",
      "Average loss at Epoch 2 and Step 1041 is: 0.238701\n",
      "Average loss at Epoch 2 and Step 1541 is: 0.233917\n",
      "Average loss at Epoch 2 and Step 2041 is: 0.237909\n",
      "Average loss at Epoch 2 and Step 2541 is: 0.220496\n",
      "Average loss at Epoch 2 and Step 3041 is: 0.229430\n",
      "Average loss at Epoch 2 and Step 3541 is: 0.231921\n",
      "Average loss at Epoch 2 and Step 4041 is: 0.236679\n",
      "Average loss at Epoch 2 and Step 4541 is: 0.226255\n",
      "Average loss at Epoch 2 and Step 5041 is: 0.234161\n",
      "Average loss at Epoch 2 and Step 5541 is: 0.246699\n",
      "Average loss at Epoch 2 and Step 6041 is: 0.235241\n",
      "Average loss at Epoch 2 and Step 6541 is: 0.227272\n",
      "Average loss at Epoch 2 and Step 7041 is: 0.233209\n",
      "Average loss at Epoch 2 and Step 7541 is: 0.242864\n",
      "Model saved to: /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_sparse221028\n"
     ]
    }
   ],
   "source": [
    "# Run the dnn model\n",
    "num_eps = 3\n",
    "batch_size = 16\n",
    "train_steps = int(len(train_idx)/batch_size)+1\n",
    "model_name = 'dnn_sparse221028'\n",
    "\n",
    "dnn = DnnModel(LEARNING_RATE=0.001,\n",
    "               BATCH_SIZE=batch_size,\n",
    "               EVA_STEP=500,\n",
    "               SAVE_STEP=train_steps,\n",
    "               NUM_EPOCHS=num_eps,\n",
    "               BETA=0.00000001,\n",
    "               KEEP_PROB = 0.7)\n",
    "\n",
    "dnn.dnn_train_sparseOnly(vecs_train,\n",
    "                      trainY,\n",
    "                      root_path+'objects/'+model_name,\n",
    "                      SPARSE_HIDDEN_DIM1=500, \n",
    "                      REGULARIZATION=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a77bce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_sparse221028-23937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_sparse221028-23937\n",
      "2022-10-28 23:20:34.014039: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.23965509\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "modelidx = int((train_steps * num_eps))\n",
    "probs = dnn.dnn_eval_sparseOnly(vecs_test,\n",
    "                     testY,\n",
    "                     root_path+'objects/',\n",
    "                     model_name+'-'+str(modelidx)+'.meta')\n",
    "preds = np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a5c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     28837\n",
      "           1       0.86      0.50      0.63      3077\n",
      "\n",
      "    accuracy                           0.94     31914\n",
      "   macro avg       0.90      0.74      0.80     31914\n",
      "weighted avg       0.94      0.94      0.94     31914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([el[1] for el in testY], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa24c3",
   "metadata": {},
   "source": [
    "### Remark\n",
    "This custom-built model produces decent results compared with the pre-trained transfer learning model above. It f1-score on the positive set is 0.63, a bit less than the previous model's 0.65. It's f1-score on the negative set 0.97 is the same as the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a22aeb",
   "metadata": {},
   "source": [
    "## Propose a improvement strategy, build the model and compare the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffcf1d",
   "metadata": {},
   "source": [
    "#### The strategy I propose here is combining the sparse (TFIDF) and dense (Embedding from the pre-trained model) features. To do so, the module 'dnn_dense_plus_sparse_module' I built will \n",
    "1. create neural network layers on top of the sparse and dense features respectively,\n",
    "2. two latent vectors (from sparse and dense layers respectively) will be concatenated, and \n",
    "3. build neural network layers on top of the concatenated features for the final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973c0e5",
   "metadata": {},
   "source": [
    "### Model Achitecture\n",
    "\n",
    "The sparse layer is similary as before: TFIDF Input -> Hidden Layer (SPARSE_HIDDEN_DIM1=500 specified below) -> Hidden Layer (SPARSE_HIDDEN_DIM2=200 specified below) \n",
    "\n",
    "The dense layser is: Embedding Input -> Hidden Layer (DENSE_HIDDEN_DIM1=500) -> Hidden Layer (DENSE_HIDDEN_DIM1=200)\n",
    "\n",
    "Concatenate: Concatenate the sparse and dense layer (dim==400) -> Hidden Layer (CONCAT_HIDDEN_DIM=200) -> Logits (dim==2) -> Output Layer (by softmax, dim==2). \n",
    "\n",
    "Note: This model structure is defined in dnn_dense_plus_sparse_module.dnn_train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82bf29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dense embedding from the pre-trained model\n",
    "Embed = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
    "embed = hub.load(Embed)\n",
    "emb = embed(ds.text.to_numpy())\n",
    "emb = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6430a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pre-trained TFIDF Vectorizer\n",
      "Get The TFIDF of Texts\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dnn_dense_plus_sparse_module import DnnModel\n",
    "from dnn_dense_plus_sparse_module import CreateFeatures\n",
    "root_path = '/Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/'\n",
    "\n",
    "# Use the previouly generated TFIDF model to generate the sparse (TFIDF) features\n",
    "cf = CreateFeatures()\n",
    "#cf.tfidf_fit(ds.text, root_path + '/objects/tfidf.pkl')\n",
    "vecs = cf.generate_tfidf_from_text(ds.text, root_path + '/objects/tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21944416",
   "metadata": {},
   "source": [
    "Prepare Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5a2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot target variables\n",
    "targets = (pd.get_dummies(ds.label)).to_numpy()\n",
    "\n",
    "# Split train and test\n",
    "import random\n",
    "random.seed(0)\n",
    "n = len(ds)\n",
    "test_idx = sorted(random.sample(range(n), int(0.2*n)))\n",
    "train_idx = sorted(list(set(range(n)) - set(test_idx)))\n",
    "\n",
    "emb_train = emb[train_idx]\n",
    "emb_test = emb[test_idx]\n",
    "vecs_train = vecs[train_idx]\n",
    "vecs_test = vecs[test_idx]\n",
    "trainY = targets[train_idx, :]\n",
    "testY = targets[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d05f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph started running...\n",
      "There will be 100 epochs. Each eopch will have 7979 steps.\n",
      "Average loss at Epoch 0 and Step 1999 is: 30.119905\n",
      "Average loss at Epoch 0 and Step 3999 is: 11.324261\n",
      "Average loss at Epoch 0 and Step 5999 is: 9.890114\n",
      "Average loss at Epoch 1 and Step 20 is: 0.074699\n",
      "Average loss at Epoch 1 and Step 2020 is: 8.184718\n",
      "Average loss at Epoch 1 and Step 4020 is: 7.241530\n",
      "Average loss at Epoch 1 and Step 6020 is: 6.760329\n",
      "Average loss at Epoch 2 and Step 41 is: 0.118724\n",
      "Average loss at Epoch 2 and Step 2041 is: 5.586884\n",
      "Average loss at Epoch 2 and Step 4041 is: 5.027705\n",
      "Average loss at Epoch 2 and Step 6041 is: 4.643208\n",
      "Average loss at Epoch 3 and Step 62 is: 0.117959\n",
      "Average loss at Epoch 3 and Step 2062 is: 3.911491\n",
      "Average loss at Epoch 3 and Step 4062 is: 3.385349\n",
      "Average loss at Epoch 3 and Step 6062 is: 3.202513\n",
      "Average loss at Epoch 4 and Step 83 is: 0.116218\n",
      "Average loss at Epoch 4 and Step 2083 is: 2.651119\n",
      "Average loss at Epoch 4 and Step 4083 is: 2.307343\n",
      "Average loss at Epoch 4 and Step 6083 is: 2.137114\n",
      "Average loss at Epoch 5 and Step 104 is: 0.116497\n",
      "Average loss at Epoch 5 and Step 2104 is: 1.732274\n",
      "Average loss at Epoch 5 and Step 4104 is: 1.510756\n",
      "Average loss at Epoch 5 and Step 6104 is: 1.402991\n",
      "Average loss at Epoch 6 and Step 125 is: 0.077322\n",
      "Average loss at Epoch 6 and Step 2125 is: 1.125880\n",
      "Average loss at Epoch 6 and Step 4125 is: 0.969855\n",
      "Average loss at Epoch 6 and Step 6125 is: 0.900485\n",
      "Average loss at Epoch 7 and Step 146 is: 0.062321\n",
      "Average loss at Epoch 7 and Step 2146 is: 0.767132\n",
      "Average loss at Epoch 7 and Step 4146 is: 0.692088\n",
      "Average loss at Epoch 7 and Step 6146 is: 0.703249\n",
      "Average loss at Epoch 8 and Step 167 is: 0.054383\n",
      "Average loss at Epoch 8 and Step 2167 is: 0.651599\n",
      "Average loss at Epoch 8 and Step 4167 is: 0.616540\n",
      "Average loss at Epoch 8 and Step 6167 is: 0.613723\n",
      "Average loss at Epoch 9 and Step 188 is: 0.064059\n",
      "Average loss at Epoch 9 and Step 2188 is: 0.590152\n",
      "Average loss at Epoch 9 and Step 4188 is: 0.582309\n",
      "Average loss at Epoch 9 and Step 6188 is: 0.587359\n",
      "Average loss at Epoch 10 and Step 209 is: 0.062139\n",
      "Average loss at Epoch 10 and Step 2209 is: 0.545174\n",
      "Average loss at Epoch 10 and Step 4209 is: 0.517965\n",
      "Average loss at Epoch 10 and Step 6209 is: 0.542959\n",
      "Average loss at Epoch 11 and Step 230 is: 0.064207\n",
      "Average loss at Epoch 11 and Step 2230 is: 0.508210\n",
      "Average loss at Epoch 11 and Step 4230 is: 0.498819\n",
      "Average loss at Epoch 11 and Step 6230 is: 0.496751\n",
      "Average loss at Epoch 12 and Step 251 is: 0.068265\n",
      "Average loss at Epoch 12 and Step 2251 is: 0.470732\n",
      "Average loss at Epoch 12 and Step 4251 is: 0.455742\n",
      "Average loss at Epoch 12 and Step 6251 is: 0.462774\n",
      "Average loss at Epoch 13 and Step 272 is: 0.065134\n",
      "Average loss at Epoch 13 and Step 2272 is: 0.433370\n",
      "Average loss at Epoch 13 and Step 4272 is: 0.427364\n",
      "Average loss at Epoch 13 and Step 6272 is: 0.433149\n",
      "Average loss at Epoch 14 and Step 293 is: 0.067690\n",
      "Average loss at Epoch 14 and Step 2293 is: 0.416675\n",
      "Average loss at Epoch 14 and Step 4293 is: 0.396073\n",
      "Average loss at Epoch 14 and Step 6293 is: 0.403964\n",
      "Average loss at Epoch 15 and Step 314 is: 0.061970\n",
      "Average loss at Epoch 15 and Step 2314 is: 0.394019\n",
      "Average loss at Epoch 15 and Step 4314 is: 0.373503\n",
      "Average loss at Epoch 15 and Step 6314 is: 0.385051\n",
      "Average loss at Epoch 16 and Step 335 is: 0.065554\n",
      "Average loss at Epoch 16 and Step 2335 is: 0.368536\n",
      "Average loss at Epoch 16 and Step 4335 is: 0.356240\n",
      "Average loss at Epoch 16 and Step 6335 is: 0.372414\n",
      "Average loss at Epoch 17 and Step 356 is: 0.066638\n",
      "Average loss at Epoch 17 and Step 2356 is: 0.352918\n",
      "Average loss at Epoch 17 and Step 4356 is: 0.331756\n",
      "Average loss at Epoch 17 and Step 6356 is: 0.359415\n",
      "Average loss at Epoch 18 and Step 377 is: 0.064047\n",
      "Average loss at Epoch 18 and Step 2377 is: 0.329555\n",
      "Average loss at Epoch 18 and Step 4377 is: 0.315221\n",
      "Average loss at Epoch 18 and Step 6377 is: 0.337208\n",
      "Average loss at Epoch 19 and Step 398 is: 0.065299\n",
      "Average loss at Epoch 19 and Step 2398 is: 0.315137\n",
      "Average loss at Epoch 19 and Step 4398 is: 0.302338\n",
      "Average loss at Epoch 19 and Step 6398 is: 0.319654\n",
      "Average loss at Epoch 20 and Step 419 is: 0.068134\n",
      "Average loss at Epoch 20 and Step 2419 is: 0.298323\n",
      "Average loss at Epoch 20 and Step 4419 is: 0.290261\n",
      "Average loss at Epoch 20 and Step 6419 is: 0.294920\n",
      "Average loss at Epoch 21 and Step 440 is: 0.071762\n",
      "Average loss at Epoch 21 and Step 2440 is: 0.282609\n",
      "Average loss at Epoch 21 and Step 4440 is: 0.277871\n",
      "Average loss at Epoch 21 and Step 6440 is: 0.288487\n",
      "Average loss at Epoch 22 and Step 461 is: 0.071300\n",
      "Average loss at Epoch 22 and Step 2461 is: 0.274317\n",
      "Average loss at Epoch 22 and Step 4461 is: 0.266756\n",
      "Average loss at Epoch 22 and Step 6461 is: 0.267462\n",
      "Average loss at Epoch 23 and Step 482 is: 0.065209\n",
      "Average loss at Epoch 23 and Step 2482 is: 0.270279\n",
      "Average loss at Epoch 23 and Step 4482 is: 0.264653\n",
      "Average loss at Epoch 23 and Step 6482 is: 0.271260\n",
      "Average loss at Epoch 24 and Step 503 is: 0.064107\n",
      "Average loss at Epoch 24 and Step 2503 is: 0.245753\n",
      "Average loss at Epoch 24 and Step 4503 is: 0.256183\n",
      "Average loss at Epoch 24 and Step 6503 is: 0.258176\n",
      "Average loss at Epoch 25 and Step 524 is: 0.066907\n",
      "Average loss at Epoch 25 and Step 2524 is: 0.241501\n",
      "Average loss at Epoch 25 and Step 4524 is: 0.238371\n",
      "Average loss at Epoch 25 and Step 6524 is: 0.249410\n",
      "Average loss at Epoch 26 and Step 545 is: 0.065962\n",
      "Average loss at Epoch 26 and Step 2545 is: 0.231272\n",
      "Average loss at Epoch 26 and Step 4545 is: 0.225877\n",
      "Average loss at Epoch 26 and Step 6545 is: 0.242528\n",
      "Average loss at Epoch 27 and Step 566 is: 0.067262\n",
      "Average loss at Epoch 27 and Step 2566 is: 0.228775\n",
      "Average loss at Epoch 27 and Step 4566 is: 0.222171\n",
      "Average loss at Epoch 27 and Step 6566 is: 0.227115\n",
      "Average loss at Epoch 28 and Step 587 is: 0.067612\n",
      "Average loss at Epoch 28 and Step 2587 is: 0.220339\n",
      "Average loss at Epoch 28 and Step 4587 is: 0.212737\n",
      "Average loss at Epoch 28 and Step 6587 is: 0.227789\n",
      "Average loss at Epoch 29 and Step 608 is: 0.071050\n",
      "Average loss at Epoch 29 and Step 2608 is: 0.219927\n",
      "Average loss at Epoch 29 and Step 4608 is: 0.211581\n",
      "Average loss at Epoch 29 and Step 6608 is: 0.213636\n",
      "Average loss at Epoch 30 and Step 629 is: 0.065331\n",
      "Average loss at Epoch 30 and Step 2629 is: 0.196547\n",
      "Average loss at Epoch 30 and Step 4629 is: 0.208379\n",
      "Average loss at Epoch 30 and Step 6629 is: 0.211869\n",
      "Average loss at Epoch 31 and Step 650 is: 0.062230\n",
      "Average loss at Epoch 31 and Step 2650 is: 0.200617\n",
      "Average loss at Epoch 31 and Step 4650 is: 0.200347\n",
      "Average loss at Epoch 31 and Step 6650 is: 0.201797\n",
      "Average loss at Epoch 32 and Step 671 is: 0.067256\n",
      "Average loss at Epoch 32 and Step 2671 is: 0.196447\n",
      "Average loss at Epoch 32 and Step 4671 is: 0.188865\n",
      "Average loss at Epoch 32 and Step 6671 is: 0.204858\n",
      "Average loss at Epoch 33 and Step 692 is: 0.065565\n",
      "Average loss at Epoch 33 and Step 2692 is: 0.190233\n",
      "Average loss at Epoch 33 and Step 4692 is: 0.191667\n",
      "Average loss at Epoch 33 and Step 6692 is: 0.195875\n",
      "Average loss at Epoch 34 and Step 713 is: 0.069016\n",
      "Average loss at Epoch 34 and Step 2713 is: 0.177384\n",
      "Average loss at Epoch 34 and Step 4713 is: 0.189372\n",
      "Average loss at Epoch 34 and Step 6713 is: 0.188673\n",
      "Average loss at Epoch 35 and Step 734 is: 0.069611\n",
      "Average loss at Epoch 35 and Step 2734 is: 0.184967\n",
      "Average loss at Epoch 35 and Step 4734 is: 0.176510\n",
      "Average loss at Epoch 35 and Step 6734 is: 0.183748\n",
      "Average loss at Epoch 36 and Step 755 is: 0.072736\n",
      "Average loss at Epoch 36 and Step 2755 is: 0.180357\n",
      "Average loss at Epoch 36 and Step 4755 is: 0.186388\n",
      "Average loss at Epoch 36 and Step 6755 is: 0.178012\n",
      "Average loss at Epoch 37 and Step 776 is: 0.071025\n",
      "Average loss at Epoch 37 and Step 2776 is: 0.181171\n",
      "Average loss at Epoch 37 and Step 4776 is: 0.174366\n",
      "Average loss at Epoch 37 and Step 6776 is: 0.173938\n",
      "Average loss at Epoch 38 and Step 797 is: 0.071279\n",
      "Average loss at Epoch 38 and Step 2797 is: 0.179781\n",
      "Average loss at Epoch 38 and Step 4797 is: 0.171680\n",
      "Average loss at Epoch 38 and Step 6797 is: 0.172536\n",
      "Average loss at Epoch 39 and Step 818 is: 0.069217\n",
      "Average loss at Epoch 39 and Step 2818 is: 0.176849\n",
      "Average loss at Epoch 39 and Step 4818 is: 0.168951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at Epoch 39 and Step 6818 is: 0.176280\n",
      "Average loss at Epoch 40 and Step 839 is: 0.064223\n",
      "Average loss at Epoch 40 and Step 2839 is: 0.169233\n",
      "Average loss at Epoch 40 and Step 4839 is: 0.162135\n",
      "Average loss at Epoch 40 and Step 6839 is: 0.170222\n",
      "Average loss at Epoch 41 and Step 860 is: 0.075163\n",
      "Average loss at Epoch 41 and Step 2860 is: 0.166058\n",
      "Average loss at Epoch 41 and Step 4860 is: 0.156781\n",
      "Average loss at Epoch 41 and Step 6860 is: 0.170938\n",
      "Average loss at Epoch 42 and Step 881 is: 0.071023\n",
      "Average loss at Epoch 42 and Step 2881 is: 0.164006\n",
      "Average loss at Epoch 42 and Step 4881 is: 0.165172\n",
      "Average loss at Epoch 42 and Step 6881 is: 0.166514\n",
      "Average loss at Epoch 43 and Step 902 is: 0.072328\n",
      "Average loss at Epoch 43 and Step 2902 is: 0.158829\n",
      "Average loss at Epoch 43 and Step 4902 is: 0.158715\n",
      "Average loss at Epoch 43 and Step 6902 is: 0.163570\n",
      "Average loss at Epoch 44 and Step 923 is: 0.077420\n",
      "Average loss at Epoch 44 and Step 2923 is: 0.147155\n",
      "Average loss at Epoch 44 and Step 4923 is: 0.156431\n",
      "Average loss at Epoch 44 and Step 6923 is: 0.156209\n",
      "Average loss at Epoch 45 and Step 944 is: 0.068606\n",
      "Average loss at Epoch 45 and Step 2944 is: 0.168421\n",
      "Average loss at Epoch 45 and Step 4944 is: 0.162082\n",
      "Average loss at Epoch 45 and Step 6944 is: 0.154199\n",
      "Average loss at Epoch 46 and Step 965 is: 0.084627\n",
      "Average loss at Epoch 46 and Step 2965 is: 0.153301\n",
      "Average loss at Epoch 46 and Step 4965 is: 0.146830\n",
      "Average loss at Epoch 46 and Step 6965 is: 0.152111\n",
      "Average loss at Epoch 47 and Step 986 is: 0.071483\n",
      "Average loss at Epoch 47 and Step 2986 is: 0.145310\n",
      "Average loss at Epoch 47 and Step 4986 is: 0.147066\n",
      "Average loss at Epoch 47 and Step 6986 is: 0.150373\n",
      "Average loss at Epoch 48 and Step 1007 is: 0.075831\n",
      "Average loss at Epoch 48 and Step 3007 is: 0.141664\n",
      "Average loss at Epoch 48 and Step 5007 is: 0.144632\n",
      "Average loss at Epoch 48 and Step 7007 is: 0.158125\n",
      "Average loss at Epoch 49 and Step 1028 is: 0.084377\n",
      "Average loss at Epoch 49 and Step 3028 is: 0.147013\n",
      "Average loss at Epoch 49 and Step 5028 is: 0.150844\n",
      "Average loss at Epoch 49 and Step 7028 is: 0.154120\n",
      "Model saved to: /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_dense_plus_sparse221028\n",
      "Average loss at Epoch 50 and Step 1049 is: 0.076030\n",
      "Average loss at Epoch 50 and Step 3049 is: 0.158065\n",
      "Average loss at Epoch 50 and Step 5049 is: 0.157357\n",
      "Average loss at Epoch 50 and Step 7049 is: 0.138475\n",
      "Average loss at Epoch 51 and Step 1070 is: 0.079367\n",
      "Average loss at Epoch 51 and Step 3070 is: 0.137090\n",
      "Average loss at Epoch 51 and Step 5070 is: 0.139958\n",
      "Average loss at Epoch 51 and Step 7070 is: 0.137495\n",
      "Average loss at Epoch 52 and Step 1091 is: 0.076567\n",
      "Average loss at Epoch 52 and Step 3091 is: 0.136061\n",
      "Average loss at Epoch 52 and Step 5091 is: 0.144300\n",
      "Average loss at Epoch 52 and Step 7091 is: 0.147095\n",
      "Average loss at Epoch 53 and Step 1112 is: 0.086940\n",
      "Average loss at Epoch 53 and Step 3112 is: 0.140936\n",
      "Average loss at Epoch 53 and Step 5112 is: 0.152405\n",
      "Average loss at Epoch 53 and Step 7112 is: 0.151408\n",
      "Average loss at Epoch 54 and Step 1133 is: 0.085904\n",
      "Average loss at Epoch 54 and Step 3133 is: 0.139665\n",
      "Average loss at Epoch 54 and Step 5133 is: 0.154261\n",
      "Average loss at Epoch 54 and Step 7133 is: 0.141387\n",
      "Average loss at Epoch 55 and Step 1154 is: 0.082595\n",
      "Average loss at Epoch 55 and Step 3154 is: 0.138521\n",
      "Average loss at Epoch 55 and Step 5154 is: 0.139960\n",
      "Average loss at Epoch 55 and Step 7154 is: 0.140772\n",
      "Average loss at Epoch 56 and Step 1175 is: 0.081932\n",
      "Average loss at Epoch 56 and Step 3175 is: 0.141618\n",
      "Average loss at Epoch 56 and Step 5175 is: 0.150200\n",
      "Average loss at Epoch 56 and Step 7175 is: 0.146475\n",
      "Average loss at Epoch 57 and Step 1196 is: 0.083159\n",
      "Average loss at Epoch 57 and Step 3196 is: 0.135473\n",
      "Average loss at Epoch 57 and Step 5196 is: 0.135901\n",
      "Average loss at Epoch 57 and Step 7196 is: 0.154092\n",
      "Average loss at Epoch 58 and Step 1217 is: 0.080549\n",
      "Average loss at Epoch 58 and Step 3217 is: 0.137004\n",
      "Average loss at Epoch 58 and Step 5217 is: 0.141481\n",
      "Average loss at Epoch 58 and Step 7217 is: 0.139534\n",
      "Average loss at Epoch 59 and Step 1238 is: 0.088663\n",
      "Average loss at Epoch 59 and Step 3238 is: 0.139447\n",
      "Average loss at Epoch 59 and Step 5238 is: 0.143580\n",
      "Average loss at Epoch 59 and Step 7238 is: 0.144620\n",
      "Average loss at Epoch 60 and Step 1259 is: 0.087604\n",
      "Average loss at Epoch 60 and Step 3259 is: 0.138871\n",
      "Average loss at Epoch 60 and Step 5259 is: 0.143334\n",
      "Average loss at Epoch 60 and Step 7259 is: 0.142514\n",
      "Average loss at Epoch 61 and Step 1280 is: 0.093047\n",
      "Average loss at Epoch 61 and Step 3280 is: 0.142440\n",
      "Average loss at Epoch 61 and Step 5280 is: 0.139537\n",
      "Average loss at Epoch 61 and Step 7280 is: 0.128582\n",
      "Average loss at Epoch 62 and Step 1301 is: 0.093578\n",
      "Average loss at Epoch 62 and Step 3301 is: 0.139362\n",
      "Average loss at Epoch 62 and Step 5301 is: 0.152340\n",
      "Average loss at Epoch 62 and Step 7301 is: 0.146794\n",
      "Average loss at Epoch 63 and Step 1322 is: 0.091091\n",
      "Average loss at Epoch 63 and Step 3322 is: 0.132785\n",
      "Average loss at Epoch 63 and Step 5322 is: 0.134556\n",
      "Average loss at Epoch 63 and Step 7322 is: 0.139539\n",
      "Average loss at Epoch 64 and Step 1343 is: 0.103437\n",
      "Average loss at Epoch 64 and Step 3343 is: 0.135898\n",
      "Average loss at Epoch 64 and Step 5343 is: 0.134910\n",
      "Average loss at Epoch 64 and Step 7343 is: 0.142803\n",
      "Average loss at Epoch 65 and Step 1364 is: 0.101297\n",
      "Average loss at Epoch 65 and Step 3364 is: 0.130570\n",
      "Average loss at Epoch 65 and Step 5364 is: 0.139075\n",
      "Average loss at Epoch 65 and Step 7364 is: 0.134118\n",
      "Average loss at Epoch 66 and Step 1385 is: 0.103228\n",
      "Average loss at Epoch 66 and Step 3385 is: 0.136903\n",
      "Average loss at Epoch 66 and Step 5385 is: 0.144810\n",
      "Average loss at Epoch 66 and Step 7385 is: 0.136677\n",
      "Average loss at Epoch 67 and Step 1406 is: 0.101372\n",
      "Average loss at Epoch 67 and Step 3406 is: 0.130351\n",
      "Average loss at Epoch 67 and Step 5406 is: 0.148988\n",
      "Average loss at Epoch 67 and Step 7406 is: 0.128349\n",
      "Average loss at Epoch 68 and Step 1427 is: 0.104132\n",
      "Average loss at Epoch 68 and Step 3427 is: 0.137281\n",
      "Average loss at Epoch 68 and Step 5427 is: 0.141463\n",
      "Average loss at Epoch 68 and Step 7427 is: 0.138030\n",
      "Average loss at Epoch 69 and Step 1448 is: 0.106152\n",
      "Average loss at Epoch 69 and Step 3448 is: 0.121077\n",
      "Average loss at Epoch 69 and Step 5448 is: 0.146213\n",
      "Average loss at Epoch 69 and Step 7448 is: 0.125321\n",
      "Average loss at Epoch 70 and Step 1469 is: 0.099330\n",
      "Average loss at Epoch 70 and Step 3469 is: 0.126998\n",
      "Average loss at Epoch 70 and Step 5469 is: 0.135472\n",
      "Average loss at Epoch 70 and Step 7469 is: 0.139318\n",
      "Average loss at Epoch 71 and Step 1490 is: 0.095707\n",
      "Average loss at Epoch 71 and Step 3490 is: 0.124589\n",
      "Average loss at Epoch 71 and Step 5490 is: 0.149890\n",
      "Average loss at Epoch 71 and Step 7490 is: 0.128152\n",
      "Average loss at Epoch 72 and Step 1511 is: 0.102894\n",
      "Average loss at Epoch 72 and Step 3511 is: 0.124144\n",
      "Average loss at Epoch 72 and Step 5511 is: 0.153339\n",
      "Average loss at Epoch 72 and Step 7511 is: 0.138204\n",
      "Average loss at Epoch 73 and Step 1532 is: 0.097858\n",
      "Average loss at Epoch 73 and Step 3532 is: 0.124347\n",
      "Average loss at Epoch 73 and Step 5532 is: 0.151455\n",
      "Average loss at Epoch 73 and Step 7532 is: 0.140911\n",
      "Average loss at Epoch 74 and Step 1553 is: 0.106099\n",
      "Average loss at Epoch 74 and Step 3553 is: 0.129738\n",
      "Average loss at Epoch 74 and Step 5553 is: 0.150659\n",
      "Average loss at Epoch 74 and Step 7553 is: 0.135421\n",
      "Average loss at Epoch 75 and Step 1574 is: 0.119416\n",
      "Average loss at Epoch 75 and Step 3574 is: 0.128207\n",
      "Average loss at Epoch 75 and Step 5574 is: 0.145645\n",
      "Average loss at Epoch 75 and Step 7574 is: 0.129093\n",
      "Average loss at Epoch 76 and Step 1595 is: 0.119877\n",
      "Average loss at Epoch 76 and Step 3595 is: 0.132774\n",
      "Average loss at Epoch 76 and Step 5595 is: 0.147272\n",
      "Average loss at Epoch 76 and Step 7595 is: 0.138694\n",
      "Average loss at Epoch 77 and Step 1616 is: 0.109467\n",
      "Average loss at Epoch 77 and Step 3616 is: 0.121507\n",
      "Average loss at Epoch 77 and Step 5616 is: 0.134348\n",
      "Average loss at Epoch 77 and Step 7616 is: 0.129928\n",
      "Average loss at Epoch 78 and Step 1637 is: 0.115971\n",
      "Average loss at Epoch 78 and Step 3637 is: 0.118501\n",
      "Average loss at Epoch 78 and Step 5637 is: 0.150635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at Epoch 78 and Step 7637 is: 0.124558\n",
      "Average loss at Epoch 79 and Step 1658 is: 0.110349\n",
      "Average loss at Epoch 79 and Step 3658 is: 0.120801\n",
      "Average loss at Epoch 79 and Step 5658 is: 0.168154\n",
      "Average loss at Epoch 79 and Step 7658 is: 0.132623\n",
      "Average loss at Epoch 80 and Step 1679 is: 0.122042\n",
      "Average loss at Epoch 80 and Step 3679 is: 0.127349\n",
      "Average loss at Epoch 80 and Step 5679 is: 0.149564\n",
      "Average loss at Epoch 80 and Step 7679 is: 0.134376\n",
      "Average loss at Epoch 81 and Step 1700 is: 0.123229\n",
      "Average loss at Epoch 81 and Step 3700 is: 0.123528\n",
      "Average loss at Epoch 81 and Step 5700 is: 0.157738\n",
      "Average loss at Epoch 81 and Step 7700 is: 0.131207\n",
      "Average loss at Epoch 82 and Step 1721 is: 0.115307\n",
      "Average loss at Epoch 82 and Step 3721 is: 0.121630\n",
      "Average loss at Epoch 82 and Step 5721 is: 0.140189\n",
      "Average loss at Epoch 82 and Step 7721 is: 0.129195\n",
      "Average loss at Epoch 83 and Step 1742 is: 0.129241\n",
      "Average loss at Epoch 83 and Step 3742 is: 0.112733\n",
      "Average loss at Epoch 83 and Step 5742 is: 0.138078\n",
      "Average loss at Epoch 83 and Step 7742 is: 0.119165\n",
      "Average loss at Epoch 84 and Step 1763 is: 0.128128\n",
      "Average loss at Epoch 84 and Step 3763 is: 0.115577\n",
      "Average loss at Epoch 84 and Step 5763 is: 0.145686\n",
      "Average loss at Epoch 84 and Step 7763 is: 0.130475\n",
      "Average loss at Epoch 85 and Step 1784 is: 0.130186\n",
      "Average loss at Epoch 85 and Step 3784 is: 0.120098\n",
      "Average loss at Epoch 85 and Step 5784 is: 0.154585\n",
      "Average loss at Epoch 85 and Step 7784 is: 0.139927\n",
      "Average loss at Epoch 86 and Step 1805 is: 0.130357\n",
      "Average loss at Epoch 86 and Step 3805 is: 0.129219\n",
      "Average loss at Epoch 86 and Step 5805 is: 0.140692\n",
      "Average loss at Epoch 86 and Step 7805 is: 0.133922\n",
      "Average loss at Epoch 87 and Step 1826 is: 0.132534\n",
      "Average loss at Epoch 87 and Step 3826 is: 0.123058\n",
      "Average loss at Epoch 87 and Step 5826 is: 0.143291\n",
      "Average loss at Epoch 87 and Step 7826 is: 0.124557\n",
      "Average loss at Epoch 88 and Step 1847 is: 0.129825\n",
      "Average loss at Epoch 88 and Step 3847 is: 0.118255\n",
      "Average loss at Epoch 88 and Step 5847 is: 0.159174\n",
      "Average loss at Epoch 88 and Step 7847 is: 0.129472\n",
      "Average loss at Epoch 89 and Step 1868 is: 0.134347\n",
      "Average loss at Epoch 89 and Step 3868 is: 0.122780\n",
      "Average loss at Epoch 89 and Step 5868 is: 0.154141\n",
      "Average loss at Epoch 89 and Step 7868 is: 0.139222\n",
      "Average loss at Epoch 90 and Step 1889 is: 0.129749\n",
      "Average loss at Epoch 90 and Step 3889 is: 0.123839\n",
      "Average loss at Epoch 90 and Step 5889 is: 0.137155\n",
      "Average loss at Epoch 90 and Step 7889 is: 0.138613\n",
      "Average loss at Epoch 91 and Step 1910 is: 0.134850\n",
      "Average loss at Epoch 91 and Step 3910 is: 0.130005\n",
      "Average loss at Epoch 91 and Step 5910 is: 0.145687\n",
      "Average loss at Epoch 91 and Step 7910 is: 0.130048\n",
      "Average loss at Epoch 92 and Step 1931 is: 0.135556\n",
      "Average loss at Epoch 92 and Step 3931 is: 0.126152\n",
      "Average loss at Epoch 92 and Step 5931 is: 0.148085\n",
      "Average loss at Epoch 92 and Step 7931 is: 0.127773\n",
      "Average loss at Epoch 93 and Step 1952 is: 0.125653\n",
      "Average loss at Epoch 93 and Step 3952 is: 0.127067\n",
      "Average loss at Epoch 93 and Step 5952 is: 0.137532\n",
      "Average loss at Epoch 93 and Step 7952 is: 0.128019\n",
      "Average loss at Epoch 94 and Step 1973 is: 0.149966\n",
      "Average loss at Epoch 94 and Step 3973 is: 0.112265\n",
      "Average loss at Epoch 94 and Step 5973 is: 0.148853\n",
      "Average loss at Epoch 94 and Step 7973 is: 0.132280\n",
      "Average loss at Epoch 95 and Step 1994 is: 0.126153\n",
      "Average loss at Epoch 95 and Step 3994 is: 0.127408\n",
      "Average loss at Epoch 95 and Step 5994 is: 0.126936\n",
      "Average loss at Epoch 96 and Step 15 is: 0.000403\n",
      "Average loss at Epoch 96 and Step 2015 is: 0.129483\n",
      "Average loss at Epoch 96 and Step 4015 is: 0.113679\n",
      "Average loss at Epoch 96 and Step 6015 is: 0.141162\n",
      "Average loss at Epoch 97 and Step 36 is: 0.002024\n",
      "Average loss at Epoch 97 and Step 2036 is: 0.136779\n",
      "Average loss at Epoch 97 and Step 4036 is: 0.112107\n",
      "Average loss at Epoch 97 and Step 6036 is: 0.147850\n",
      "Average loss at Epoch 98 and Step 57 is: 0.002284\n",
      "Average loss at Epoch 98 and Step 2057 is: 0.137294\n",
      "Average loss at Epoch 98 and Step 4057 is: 0.126796\n",
      "Average loss at Epoch 98 and Step 6057 is: 0.138996\n",
      "Average loss at Epoch 99 and Step 78 is: 0.007108\n",
      "Average loss at Epoch 99 and Step 2078 is: 0.141039\n",
      "Average loss at Epoch 99 and Step 4078 is: 0.113447\n",
      "Average loss at Epoch 99 and Step 6078 is: 0.134211\n",
      "Model saved to: /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_dense_plus_sparse221028\n"
     ]
    }
   ],
   "source": [
    "# Run the dnn model\n",
    "num_eps = 100\n",
    "batch_size = 16\n",
    "train_steps = int(len(trainY)/batch_size)+1\n",
    "model_name = 'dnn_dense_plus_sparse221028'\n",
    "\n",
    "dnn = DnnModel(LEARNING_RATE=0.001,\n",
    "               BATCH_SIZE=batch_size,\n",
    "               EVA_STEP=2000,\n",
    "               SAVE_STEP=train_steps*num_eps/2,\n",
    "               NUM_EPOCHS=num_eps,\n",
    "               BETA=0.00000001,\n",
    "               KEEP_PROB = 0.7)\n",
    "\n",
    "dnn.dnn_train(emb_train,\n",
    "              vecs_train,\n",
    "              trainY,\n",
    "              root_path+'objects/'+model_name,\n",
    "              DENSE_HIDDEN_DIM1=500,\n",
    "              DENSE_HIDDEN_DIM2=200,\n",
    "              SPARSE_HIDDEN_DIM1=500, \n",
    "              SPARSE_HIDDEN_DIM2=200,\n",
    "              CONCAT_HIDDEN_DIM=200,\n",
    "              REGULARIZATION=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0f588a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_dense_plus_sparse221028-797900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jili/Library/CloudStorage/GoogleDrive-hamlet.j@gmail.com/My Drive/DS/bestegg/objects/dnn_dense_plus_sparse221028-797900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.4071696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     28837\n",
      "           1       0.77      0.69      0.73      3077\n",
      "\n",
      "    accuracy                           0.95     31914\n",
      "   macro avg       0.87      0.83      0.85     31914\n",
      "weighted avg       0.95      0.95      0.95     31914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "modelidx = int((train_steps * num_eps))\n",
    "probs = dnn.dnn_eval(emb_test,\n",
    "                     vecs_test,\n",
    "                     testY,\n",
    "                     root_path+'objects/',\n",
    "                     model_name+'-'+str(modelidx)+'.meta')\n",
    "preds = np.argmax(probs, axis=1)\n",
    "print(classification_report([el[1] for el in testY], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a94104",
   "metadata": {},
   "source": [
    "### Remark\n",
    "1. As you can see above, it takes a lot epochs for this model structure to converge to a low loss, while it took only 3 epochs for the basice (sparse only) model structure to converge. \n",
    "2. This model structure generates and learns from richer features and eventually produces better predictions. \n",
    "3. The above overall perfomance is better than the pre-trained transfer learning model, and the basic model, with 0.73 f1-score on the positive set, vs 0.63-0.65 from the other models. \n",
    "4. Regularization techniques including dropout and l2 regularization are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c8cc7",
   "metadata": {},
   "source": [
    "## Discussion - Application & Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50673aac",
   "metadata": {},
   "source": [
    "1. The last custom-built model performs well. More importantly it has a wide range of applications. For example, when we need to combine structured and text data, we can use this model to take care of strutured data as dense features, and text data as either tfidf features, embedding features or both. Then the model structure can naturally combine them using neural network layers. \n",
    "2. In my work experience, I implemented such for combining structure and text data, and achieved good performance and scalability. \n",
    "3. I have mainly focus on model architetures. There are certainly areas for improvement, including data processing and modeling. \n",
    "4. I didn't include deployment pieces such as deploying as API. If desired, I can share an example of doing so.\n",
    "5. For further model improvement, I would try a simple model, especially SVM, on the embedding features. I had experience that this type of model provides the best performance on the similar problems. \n",
    "6. In order to eventually use it in real world production, I would consider to explore the use of multiple different models. For example, logistic regression may not provide the best performance, but it may provide the best predictive probabilities and explanability. Good predictive probabilities are important for the downstreaming work. When logistic regression is not confident, the data can go to more complex models. And hence a portion of the incoming data is taken care of by logistic regression, and only another portion of the data goes to complex models. This helps reduce latency and improve accuracy. Unconfident predictions can also be routed to manual processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de696f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
